# üí° Data Pipeline ServiceNow + Hexagon com Airflow, dbt e BigQuery

Este projeto implementa um pipeline completo de dados unindo **ServiceNow ‚Üí BigQuery ‚Üí dbt ‚Üí Airflow**, com anonimiza√ß√£o e orquestra√ß√£o de fluxos ETL.  
O objetivo √© construir um pipeline demonstrando boas pr√°ticas de integra√ß√£o, modelagem e automa√ß√£o de workflows de dados.
Passo a passo das execu√ß√µes no [Gist](https://gist.github.com/cooorpse/62bffe6d1401dc768032095c5912baa8)

---

## Arquitetura do Projeto

1. **Extra√ß√£o & Anonimiza√ß√£o (Python)**
   - C√≥digo Python realiza a request na API do **ServiceNow**.
   - Dados sens√≠veis s√£o **anonimizados** com substitui√ß√µes rad√¥micas (ex: `short_description`).
   - C√≥digo e passo a passo [aqui](https://gist.github.com/cooorpse/62bffe6d1401dc768032095c5912baa8)
   - Resultado √© salvo e enviado para o **BigQuery** na camada **raw** (`dbt_servicenow.sn_incidents`).

   ![BigQuery](/assets/images/First Load BigQuery.png)

2. **Transforma√ß√£o (dbt)**
   - Utiliza **dbt** para organizar a camada de transforma√ß√£o:
     - **staging** ‚Üí limpeza, normaliza√ß√£o e aplica√ß√£o de regras de neg√≥cio (ex.: tratamento de SLA, categoriza√ß√£o via `CASE`).
     - **mart** ‚Üí modelos finais prontos para an√°lise (ex.: m√©tricas de incidentes resolvidos dentro do SLA).
   - Estrutura de schemas/datasets:
     ```
     dbt_servicenow   ‚Üí tabelas raw
     stg_servicenow   ‚Üí tabelas staging
     mart_servicenow  ‚Üí marts de neg√≥cio
     ```

3. **Orquestra√ß√£o (Airflow + Cosmos)**
   - Cria√ß√£o de DAGs com **Airflow Cosmos**:
     - `dbt_servicenow` ‚Üí executa modelos ServiceNow.
     - `dbt_hexagon` ‚Üí executa modelos Hexagon.
   - Cada DAG √© desacoplado, pois os dois dom√≠nios n√£o possuem depend√™ncias.

4. **Automa√ß√£o (API do Airflow)**
   - Al√©m de schedules (`@daily`), DAGs podem ser disparadas via API:
   ```bash
    curl -X POST 'localhost:8080/api/v2/dags/dbt_servicenow/dagRuns' \
      --header 'Content-Type: application/json' \
      --data '{"logical_date": null}'
    ```

    ![Airflow](/assets/images/curl.gif)

---

## Tecnologias Utilizadas

- Python ‚Üí requests + pandas para ingest√£o e anonimiza√ß√£o
- Google BigQuery ‚Üí camada raw e processamento anal√≠tico
- dbt Core ‚Üí transforma√ß√£o de dados em staging e marts
- Airflow (Cosmos) ‚Üí orquestra√ß√£o dos pipelines dbt
- Docker / Astro CLI ‚Üí ambiente isolado para Airflow + dbt
